{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_selection import SelectKBest, chi2, f_classif\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2830743, 79)\n"
     ]
    }
   ],
   "source": [
    "filenames = ['MachineLearningCSV/Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv', 'MachineLearningCSV/Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv', 'MachineLearningCSV/Friday-WorkingHours-Morning.pcap_ISCX.csv', 'MachineLearningCSV/Monday-WorkingHours.pcap_ISCX.csv','MachineLearningCSV/Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv', 'MachineLearningCSV/Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv', 'MachineLearningCSV/Tuesday-WorkingHours.pcap_ISCX.csv', 'MachineLearningCSV/Wednesday-workingHours.pcap_ISCX.csv']\n",
    "df_aug = pd.read_csv(filenames[0])\n",
    "for name in filenames[1:]:\n",
    "  df = pd.read_csv(name)\n",
    "  df_aug = pd.concat([df_aug, df], axis=0)\n",
    "  \n",
    "print(df_aug.shape)\n",
    "df_aug.to_csv('IDS-2017-full.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr_df(x, corr_val):\n",
    "    '''\n",
    "    Obj: Drops features that are strongly correlated to other features.\n",
    "          This lowers model complexity, and aids in generalizing the model.\n",
    "    Inputs:\n",
    "          df: features df (x)\n",
    "          corr_val: Columns are dropped relative to the corr_val input (e.g. 0.8)\n",
    "    Output: df that only includes uncorrelated features\n",
    "    '''\n",
    "\n",
    "    # Creates Correlation Matrix and Instantiates\n",
    "    corr_matrix = x.corr()\n",
    "    print(corr_matrix.shape)\n",
    "    print(len(corr_matrix.columns))\n",
    "    iters = range(len(corr_matrix.columns) -1 )\n",
    "    drop_cols = []\n",
    "\n",
    "    # Iterates through Correlation Matrix Table to find correlated columns\n",
    "    for i in iters:\n",
    "        for j in range(i):\n",
    "            item = corr_matrix.iloc[j:(j+1), (i+1):(i+2)]\n",
    "            col = item.columns\n",
    "            row = item.index\n",
    "            val = item.values\n",
    "            if abs(val) >= corr_val:\n",
    "                # Prints the correlated feature set and the corr val\n",
    "                print(col.values[0], \"|\", row.values[0], \"|\", round(val[0][0], 2))\n",
    "                drop_cols.append(col.values[0])\n",
    "                \n",
    "    \n",
    "\n",
    "    drop_cols = set(drop_cols)\n",
    "    print(drop_cols)\n",
    "    cols = [col for col in x.columns if col not in drop_cols]\n",
    "    \n",
    "    #no_drops = [i for i in range(len(corr_matrix.columns)) if i not in drops]\n",
    "    # Drops the correlated columns\n",
    "    #for i in drops:\n",
    "    #    col = x.iloc[:, (i+1):(i+2)].columns.values\n",
    "    #    x = x.drop(col, axis=1)\n",
    "   # return x\n",
    "    return x[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data ...\n",
      "Reading done\n",
      "(2830743, 78)\n",
      "Splitting ...\n",
      "Spliiting done.\n",
      "(1896597, 78)\n",
      "(934146, 78)\n",
      "(1896597,)\n",
      "(934146,)\n",
      "Correlated feature elimination\n",
      "(78, 78)\n",
      "78\n",
      " SYN Flag Count | Fwd PSH Flags | 1.0\n",
      " CWE Flag Count |  Fwd URG Flags | 1.0\n",
      " Avg Fwd Segment Size |  Fwd Packet Length Mean | 1.0\n",
      " Avg Bwd Segment Size |  Bwd Packet Length Mean | 1.0\n",
      " Fwd Header Length.1 |  Fwd Header Length | 1.0\n",
      "Subflow Fwd Packets |  Total Fwd Packets | 1.0\n",
      " Subflow Bwd Packets |  Total Backward Packets | 1.0\n",
      "{' SYN Flag Count', ' Fwd Header Length.1', ' Subflow Bwd Packets', 'Subflow Fwd Packets', ' Avg Bwd Segment Size', ' CWE Flag Count', ' Avg Fwd Segment Size'}\n",
      "Index([' Destination Port', ' Flow Duration', ' Total Fwd Packets',\n",
      "       ' Total Backward Packets', 'Total Length of Fwd Packets',\n",
      "       ' Total Length of Bwd Packets', ' Fwd Packet Length Max',\n",
      "       ' Fwd Packet Length Min', ' Fwd Packet Length Mean',\n",
      "       ' Fwd Packet Length Std', 'Bwd Packet Length Max',\n",
      "       ' Bwd Packet Length Min', ' Bwd Packet Length Mean',\n",
      "       ' Bwd Packet Length Std', 'Flow Bytes/s', ' Flow Packets/s',\n",
      "       ' Flow IAT Mean', ' Flow IAT Std', ' Flow IAT Max', ' Flow IAT Min',\n",
      "       'Fwd IAT Total', ' Fwd IAT Mean', ' Fwd IAT Std', ' Fwd IAT Max',\n",
      "       ' Fwd IAT Min', 'Bwd IAT Total', ' Bwd IAT Mean', ' Bwd IAT Std',\n",
      "       ' Bwd IAT Max', ' Bwd IAT Min', 'Fwd PSH Flags', ' Bwd PSH Flags',\n",
      "       ' Fwd URG Flags', ' Bwd URG Flags', ' Fwd Header Length',\n",
      "       ' Bwd Header Length', 'Fwd Packets/s', ' Bwd Packets/s',\n",
      "       ' Min Packet Length', ' Max Packet Length', ' Packet Length Mean',\n",
      "       ' Packet Length Std', ' Packet Length Variance', 'FIN Flag Count',\n",
      "       ' RST Flag Count', ' PSH Flag Count', ' ACK Flag Count',\n",
      "       ' URG Flag Count', ' ECE Flag Count', ' Down/Up Ratio',\n",
      "       ' Average Packet Size', 'Fwd Avg Bytes/Bulk', ' Fwd Avg Packets/Bulk',\n",
      "       ' Fwd Avg Bulk Rate', ' Bwd Avg Bytes/Bulk', ' Bwd Avg Packets/Bulk',\n",
      "       'Bwd Avg Bulk Rate', ' Subflow Fwd Bytes', ' Subflow Bwd Bytes',\n",
      "       'Init_Win_bytes_forward', ' Init_Win_bytes_backward',\n",
      "       ' act_data_pkt_fwd', ' min_seg_size_forward', 'Active Mean',\n",
      "       ' Active Std', ' Active Max', ' Active Min', 'Idle Mean', ' Idle Std',\n",
      "       ' Idle Max', ' Idle Min'],\n",
      "      dtype='object')\n",
      "(1896597, 71)\n",
      "(934146, 71)\n",
      "(1896597,)\n",
      "(934146,)\n"
     ]
    }
   ],
   "source": [
    "#Read\n",
    "print('Reading data ...')\n",
    "df = pd.read_csv('IDS-2017-full.csv',)\n",
    "print('Reading done')\n",
    "\n",
    "RANDOM_SEED = 41\n",
    "\n",
    "le = LabelEncoder()\n",
    "df[' Label'] = le.fit_transform(df[' Label'])\n",
    "\n",
    "Y = df[' Label']\n",
    "X = df.drop([' Label'], axis=1)\n",
    "\n",
    "del df\n",
    "X=X.astype('float64')\n",
    "print(X.select_dtypes('float64').shape)\n",
    "\n",
    "#Replace inf values (with NaN ?)\n",
    "X=X.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "#Fill NaN\n",
    "X.fillna(0.0, inplace=True)\n",
    "\n",
    "#Train test split\n",
    "\n",
    "print('Splitting ...')\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.33, random_state = RANDOM_SEED, stratify=Y)\n",
    "print('Spliiting done.')\n",
    "del X\n",
    "del Y\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_train.shape)\n",
    "print(Y_test.shape)\n",
    "\n",
    "# Eliminate mutually correlated feature pairs\n",
    "\n",
    "print('Correlated feature elimination')\n",
    "\n",
    "X_train = corr_df(X_train, 1.00)\n",
    "print(X_train.columns)\n",
    "X_test = X_test[X_train.columns]\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_train.shape)\n",
    "print(Y_test.shape)\n",
    "\n",
    "# Scale\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = pd.DataFrame(scaler.transform(X_train))\n",
    "X_test = pd.DataFrame(scaler.transform(X_test))\n",
    "\n",
    "#print('MI classif ...')\n",
    "#mi = np.array(mutual_info_classif(X_train, Y_train, n_neighbors=5, copy=True))\n",
    "#X_train = X_train[X_train.columns[mi.argsort()[-39:][::-1]]]\n",
    "#X_test = X_test[X_test.columns[mi.argsort()[-39:][::-1]]]\n",
    "\n",
    "#print ('Chi Square ...')\n",
    "#chi, _ = np.array(chi2(X_train, Y_train))\n",
    "#X_train = X_train[X_train.columns[chi.argsort()[-35:][::-1]]]\n",
    "#X_test = X_test[X_test.columns[chi.argsort()[-35:][::-1]]]\n",
    "\n",
    "#print('F classif')\n",
    "#chi, _ = np.array(f_classif(X_train, Y_train))\n",
    "#X_train = X_train[X_train.columns[chi.argsort()[-39:][::-1]]]\n",
    "#X_test = X_test[X_test.columns[chi.argsort()[-39:][::-1]]]\n",
    "\n",
    "#feature_indices = np.array([33, 34, 46,  4, 47,  8, 32, 48,  5,  0, 14,  1, 40, 28, 10, 17, 15,\n",
    "#       29, 12, 30, 11, 16,  9, 13,  7, 22,  3, 19, 50, 20,  2, 54, 55, 18,\n",
    "#       21, 51, 53, 23, 31,  6, 49, 37, 38, 52, 35, 24, 44, 45, 42, 41, 36,\n",
    "#       25, 39, 43, 26, 27])\n",
    "#print('Select features')\n",
    "#X_train = X_train[feature_indices[0:25]]\n",
    "#X_test = X_test[feature_indices[0:25]]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mi.argsort()[-78:][::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing SMOTE....\n"
     ]
    }
   ],
   "source": [
    "# Undersample and Oversample\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler, TomekLinks\n",
    "\n",
    "# Declare how much to sample from each class\n",
    "sampling_dict = {0: 200000}\n",
    "#sampling_dict_complex = {0:150000}\n",
    "rus = RandomUnderSampler(random_state=113, sampling_strategy=sampling_dict)\n",
    "tkl = TomekLinks(n_jobs=-1)\n",
    "X_train, Y_train = rus.fit_resample(X_train, Y_train)\n",
    "\n",
    "# Apply SMOTE\n",
    "# To reduce class imbalance perform SMOTE\n",
    "print('Performing SMOTE....')\n",
    "smote_dict = {1: 7500, 3: 30000, 5: 20000, 6: 20000, 7: 25000, 8: 1000, 9: 1000, 11: 20000, 12: 5000, 13: 1000, 14: 3000}\n",
    "#sm = SMOTE(random_state=42, sampling_strategy=smote_dict, n_jobs=-1) # this shit is quite heuristic\n",
    "sm = SMOTE(random_state=42, n_jobs=-1)\n",
    "X_train, Y_train = sm.fit_resample(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 8building tree 2 of 8\n",
      "building tree 3 of 8\n",
      "\n",
      "building tree 4 of 8\n",
      "building tree 5 of 8\n",
      "building tree 6 of 8\n",
      "building tree 7 of 8\n",
      "building tree 8 of 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   3 out of   8 | elapsed:   40.9s remaining:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done   8 out of   8 | elapsed:   45.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   8 out of   8 | elapsed:   45.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   3 out of   8 | elapsed:    1.0s remaining:    1.8s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    1.4s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    1.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00    750123\n",
      "           1       0.17      0.99      0.29       649\n",
      "           2       1.00      1.00      1.00     42249\n",
      "           3       0.98      1.00      0.99      3397\n",
      "           4       0.97      1.00      0.98     76254\n",
      "           5       0.98      0.99      0.99      1815\n",
      "           6       0.99      0.99      0.99      1913\n",
      "           7       1.00      1.00      1.00      2619\n",
      "           8       1.00      1.00      1.00         3\n",
      "           9       0.91      0.83      0.87        12\n",
      "          10       0.99      1.00      1.00     52447\n",
      "          11       1.00      1.00      1.00      1946\n",
      "          12       0.81      0.60      0.69       497\n",
      "          13       0.20      0.29      0.24         7\n",
      "          14       0.16      0.69      0.26       215\n",
      "\n",
      "    accuracy                           0.99    934146\n",
      "   macro avg       0.81      0.89      0.82    934146\n",
      "weighted avg       1.00      0.99      0.99    934146\n",
      "\n",
      "99.26199973023489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   3 out of   8 | elapsed:   10.0s remaining:   16.7s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:   11.5s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:   11.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00    200000\n",
      "           1       1.00      1.00      1.00    200000\n",
      "           2       1.00      1.00      1.00    200000\n",
      "           3       1.00      1.00      1.00    200000\n",
      "           4       1.00      1.00      1.00    200000\n",
      "           5       1.00      1.00      1.00    200000\n",
      "           6       1.00      1.00      1.00    200000\n",
      "           7       1.00      1.00      1.00    200000\n",
      "           8       1.00      1.00      1.00    200000\n",
      "           9       1.00      1.00      1.00    200000\n",
      "          10       1.00      1.00      1.00    200000\n",
      "          11       1.00      1.00      1.00    200000\n",
      "          12       0.89      0.80      0.84    200000\n",
      "          13       0.99      1.00      0.99    200000\n",
      "          14       0.82      0.91      0.86    200000\n",
      "\n",
      "    accuracy                           0.98   3000000\n",
      "   macro avg       0.98      0.98      0.98   3000000\n",
      "weighted avg       0.98      0.98      0.98   3000000\n",
      "\n",
      "97.93953333333333\n"
     ]
    }
   ],
   "source": [
    "#Training\n",
    "\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=8, verbose=2, criterion=\"gini\", n_jobs=-1)\n",
    "clf.fit(X_train,Y_train)\n",
    "\n",
    "f = open('rf.pickle','wb')\n",
    "pickle.dump(clf,f)\n",
    "f.close()\n",
    "print('Training done')\n",
    "\n",
    "def predict(x,Y,model_file):\n",
    "\n",
    "\tfile = open(model_file,'rb')\n",
    "\tmodel = pickle.load(file)\n",
    "\tfile.close()\n",
    "\n",
    "\ty_pred = model.predict(x)\n",
    "\tprint(classification_report(Y, y_pred))\n",
    "\tprint(accuracy_score(Y,y_pred)*100)\n",
    "  \n",
    "# Testing\n",
    "predict(X_test,Y_test,'rf.pickle')\n",
    "predict(X_train,Y_train,'rf.pickle')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6a10c074c5afe47b1009769a36d1ec915eed6de700089a4ad8c9756c3e5a59d1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
